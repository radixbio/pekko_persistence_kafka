include "version"

akka {
  loggers = ["akka.event.Logging$DefaultLogger"]
  loglevel = "INFO" # OFF, ERROR, WARNING, INFO, DEBUG

  actor {
    provider = "cluster" # local, remote, cluster
    allow-java-serialization = off
    enable-additional-serialization-bindings = on
    serializers = ${radix.serializers}
    serialization-bindings = ${radix.serialization-bindings}
#    debug {  #Uncomment to turn on all actor debug messages
#      receive = on
#      autoreceive = on
#      lifecycle = on
#      fsm = on
#      event-stream = on
#      unhandled = on
#    }
  }

  jvm-shutdown-hooks = on

  cluster {
    shutdown-after-unsuccessful-join-seed-nodes = 30s
    distributed-data {
      # Actor name of the Replicator actor, /system/ddataReplicator
      name = ddataReplicator

      # Replicas are running on members tagged with this role.
      # All members are used if undefined or empty.
      role = ""

      # How often the Replicator should send out gossip information
      gossip-interval = 2 s

      # How often the subscribers will be notified of changes, if any
      notify-subscribers-interval = 500 ms

      # Maximum number of entries to transfer in one gossip message when synchronizing
      # the replicas. Next chunk will be transferred in next round of gossip.
      max-delta-elements = 1000

      # The id of the dispatcher to use for Replicator actors. If not specified
      # default dispatcher is used.
      # If specified you need to define the settings of the actual dispatcher.
      use-dispatcher = ""

      # How often the Replicator checks for pruning of data associated with
      # removed cluster nodes. If this is set to 'off' the pruning feature will
      # be completely disabled.
      pruning-interval = 120 s

      # How long time it takes to spread the data to all other replica nodes.
      # This is used when initiating and completing the pruning process of data associated
      # with removed cluster nodes. The time measurement is stopped when any replica is
      # unreachable, but it's still recommended to configure this with certain margin.
      # It should be in the magnitude of minutes even though typical dissemination time
      # is shorter (grows logarithmic with number of nodes). There is no advantage of
      # setting this too low. Setting it to large value will delay the pruning process.
      max-pruning-dissemination = 300 s

      # The markers of that pruning has been performed for a removed node are kept for this
      # time and thereafter removed. If and old data entry that was never pruned is somehow
      # injected and merged with existing data after this time the value will not be correct.
      # This would be possible (although unlikely) in the case of a long network partition.
      # It should be in the magnitude of hours. For durable data it is configured by
      # 'akka.cluster.distributed-data.durable.pruning-marker-time-to-live'.
      pruning-marker-time-to-live = 6 h

      # Serialized Write and Read messages are cached when they are sent to
      # several nodes. If no further activity they are removed from the cache
      # after this duration.
      serializer-cache-time-to-live = 10s

      # Settings for delta-CRDT
      delta-crdt {
        # enable or disable delta-CRDT replication
        enabled = on

        # Some complex deltas grow in size for each update and above this
        # threshold such deltas are discarded and sent as full state instead.
        # This is number of elements or similar size hint, not size in bytes.
        max-delta-size = 200
      }

      durable {
        # List of keys that are durable. Prefix matching is supported by using * at the
        # end of a key.
        keys = ["*"]

        # The markers of that pruning has been performed for a removed node are kept for this
        # time and thereafter removed. If and old data entry that was never pruned is
        # injected and merged with existing data after this time the value will not be correct.
        # This would be possible if replica with durable data didn't participate in the pruning
        # (e.g. it was shutdown) and later started after this time. A durable replica should not
        # be stopped for longer time than this duration and if it is joining again after this
        # duration its data should first be manually removed (from the lmdb directory).
        # It should be in the magnitude of days. Note that there is a corresponding setting
        # for non-durable data: 'akka.cluster.distributed-data.pruning-marker-time-to-live'.
        pruning-marker-time-to-live = 10 d

        # Fully qualified class name of the durable store actor. It must be a subclass
        # of akka.actor.Actor and handle the protocol defined in
        # akka.cluster.ddata.DurableStore. The class must have a constructor with
        # com.typesafe.config.Config parameter.
        store-actor-class = akka.cluster.ddata.LmdbDurableStore

        use-dispatcher = akka.cluster.distributed-data.durable.pinned-store

        pinned-store {
          executor = thread-pool-executor
          type = PinnedDispatcher
        }

        # Config for the LmdbDurableStore
        lmdb {
          # Directory of LMDB file. There are two options:
          # 1. A relative or absolute path to a directory that ends with 'ddata'
          #    the full name of the directory will contain name of the ActorSystem
          #    and its remote port.
          # 2. Otherwise the path is used as is, as a relative or absolute path to
          #    a directory.
          #
          # When running in production you may want to configure this to a specific
          # path (alt 2), since the default directory contains the remote port of the
          # actor system to make the name unique. If using a dynamically assigned
          # port (0) it will be different each time and the previously stored data
          # will not be loaded.
          dir = "/tmp/ddata"

          # Size in bytes of the memory mapped file.
          map-size = 100 MiB

          # Accumulate changes before storing improves performance with the
          # risk of losing the last writes if the JVM crashes.
          # The interval is by default set to 'off' to write each update immediately.
          # Enabling write behind by specifying a duration, e.g. 200ms, is especially
          # efficient when performing many writes to the same key, because it is only
          # the last value for each key that will be serialized and stored.
          # write-behind-interval = 200 ms
          write-behind-interval = off
        }
      }
    }
  }

  coordinated-shutdown {
    exit-jvm = on
    exit-code = 0
  }

  debug {
    receive = on
    autoreceive = on
    lifecycle = on
    fsm = on
    event-stream = on
    unhandled = on
    router-misconfiguration = on
  }

  allow-java-serialization = off

  persistence {
    journal {
      plugin = "radix-journal"
    }

    snapshot-store {
      plugin = "radix-snapshot"
    }
  }
}

radix {
  kafka {
    bootstrap {
      servers = "PLAINTEXT://kafka-daemons-kafka-kafka.service.consul:9092"
      servers = "PLAINTEXT://"${?NOMAD_PREFIX}"kafka-daemons-kafka-kafka.service.consul:9092"
      servers = ${?KAFKA_BOOTSTRAP_SERVERS}
    }
  }

  avro {
    schema {
      registry {
      url = "http://kc-daemons-companions-schema-registry.service.consul:8081"
      url = "http://"${?NOMAD_PREFIX}"kc-daemons-companions-schema-registry.service.consul:8081"
      url = ${?AVRO_SCHEMA_REGISTRY_URL}
      }
    }

    key {
      subject {
        name {
          strategy = "io.confluent.kafka.serializers.subject.TopicRecordNameStrategy"
        }
      }
    }

    value {
      subject {
        name {
          strategy = "io.confluent.kafka.serializers.subject.TopicRecordNameStrategy"
        }
      }
    }
  }
  serialization-bindings {

  }
  serializers {

  }
}

radix-snapshot {
  class = "com.radix.shared.persistence.KafkaSnapshotStore"
  stream-parallelism = 10
  kafka = ${radix.kafka}
  kafka.producer = {
    # Tuning parameter of how many sends that can run in parallel.
    parallelism = 100

    # Duration to wait for `KafkaProducer.close` to finish.
    close-timeout = 60s

    # Call `KafkaProducer.close` when the stream is shutdown. This is important to override to false
    # when the producer instance is shared across multiple producer stages.
    close-on-producer-stop = false

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the producer stages. Some blocking may occur.
    # When this value is empty, the dispatcher configured for the stream
    # will be used.
    use-dispatcher = "akka.kafka.default-dispatcher"

    # The time interval to commit a transaction when using the `Transactional.sink` or `Transactional.flow`
    # for exactly-once-semantics processing.
    eos-commit-interval = 100ms

    # Properties defined by org.apache.kafka.clients.producer.ProducerConfig
    # can be defined in this configuration section.
    kafka-clients {
    }
  }
  kafka.consumer = {
    # Tuning property of scheduled polls.
    # Controls the interval from one scheduled poll to the next.
    poll-interval = 50ms

    # Tuning property of the `KafkaConsumer.poll` parameter.
    # Note that non-zero value means that the thread that
    # is executing the stage will be blocked. See also the `wakup-timeout` setting below.
    poll-timeout = 50ms

    # The stage will delay stopping the internal actor to allow processing of
    # messages already in the stream (required for successful committing).
    # Prefer use of `DrainingControl` over a large stop-timeout.
    stop-timeout = 30s

    # Duration to wait for `KafkaConsumer.close` to finish.
    close-timeout = 20s

    # If offset commit requests are not completed within this timeout
    # the returned Future is completed `CommitTimeoutException`.
    # The `Transactional.source` waits this ammount of time for the producer to mark messages as not
    # being in flight anymore as well as waiting for messages to drain, when rebalance is triggered.
    commit-timeout = 15s

    # If commits take longer than this time a warning is logged
    commit-time-warning = 1s

    # Not used anymore (since 1.0-RC1)
    # wakeup-timeout = 3s

    # Not used anymore (since 1.0-RC1)
    # max-wakeups = 10

    # If set to a finite duration, the consumer will re-send the last committed offsets periodically
    # for all assigned partitions. See https://issues.apache.org/jira/browse/KAFKA-4682.
    commit-refresh-interval = infinite

    # Not used anymore (since 1.0-RC1)
    # wakeup-debug = true

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the KafkaConsumerActor. Some blocking may occur.
    use-dispatcher = "akka.kafka.default-dispatcher"

    # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
    # can be defined in this configuration section.
    kafka-clients {
      # Disable auto-commit by default
      enable.auto.commit = false
    }

    # Time to wait for pending requests when a partition is closed
    wait-close-partition = 500ms

    # Limits the query to Kafka for a topic's position
    position-timeout = 5s

    # When using `AssignmentOffsetsForTimes` subscriptions: timeout for the
    # call to Kafka's API
    offset-for-times-timeout = 5s

    # Timeout for akka.kafka.Metadata requests
    # This value is used instead of Kafka's default from `default.api.timeout.ms`
    # which is 1 minute.
    metadata-request-timeout = 5s

    # Interval for checking that transaction was completed before closing the consumer.
    # Used in the transactional flow for exactly-once-semantics processing.
    eos-draining-check-interval = 30ms

    # Issue warnings when a call to a partition assignment handler method takes
    # longer than this.
    partition-handler-warning = 5s

    # Settings for checking the connection to the Kafka broker. Connection checking uses `listTopics` requests with the timeout
    # configured by `consumer.metadata-request-timeout`
    connection-checker {
      #Flag to turn on connection checker
      enable = true

      # Amount of attempts to be performed after a first connection failure occurs
      # Required, non-negative integer
      max-retries = 3

      # Interval for the connection check. Used as the base for exponential retry.
      check-interval = 15s

      # Check interval multiplier for backoff interval
      # Required, positive number
      backoff-factor = 2.0
    }
  }
  avro = ${radix.avro}
}

radix-journal {
  class = "com.radix.shared.persistence.KafkaJournal"
  stream-parallelism = 10
  kafka = ${radix.kafka}
  kafka.producer = {
    # Tuning parameter of how many sends that can run in parallel.
    parallelism = 100

    # Duration to wait for `KafkaProducer.close` to finish.
    close-timeout = 60s

    # Call `KafkaProducer.close` when the stream is shutdown. This is important to override to false
    # when the producer instance is shared across multiple producer stages.
    close-on-producer-stop = false
    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the producer stages. Some blocking may occur.
    # When this value is empty, the dispatcher configured for the stream
    # will be used.
    use-dispatcher = "akka.kafka.default-dispatcher"

    # The time interval to commit a transaction when using the `Transactional.sink` or `Transactional.flow`
    # for exactly-once-semantics processing.
    eos-commit-interval = 100ms

    # Properties defined by org.apache.kafka.clients.producer.ProducerConfig
    # can be defined in this configuration section.
    kafka-clients {
    }
  }
  kafka.consumer = {
    # Tuning property of scheduled polls.
    # Controls the interval from one scheduled poll to the next.
    poll-interval = 50ms

    # Tuning property of the `KafkaConsumer.poll` parameter.
    # Note that non-zero value means that the thread that
    # is executing the stage will be blocked. See also the `wakup-timeout` setting below.
    poll-timeout = 50ms

    # The stage will delay stopping the internal actor to allow processing of
    # messages already in the stream (required for successful committing).
    # Prefer use of `DrainingControl` over a large stop-timeout.
    stop-timeout = 30s

    # Duration to wait for `KafkaConsumer.close` to finish.
    close-timeout = 20s

    # If offset commit requests are not completed within this timeout
    # the returned Future is completed `CommitTimeoutException`.
    # The `Transactional.source` waits this ammount of time for the producer to mark messages as not
    # being in flight anymore as well as waiting for messages to drain, when rebalance is triggered.
    commit-timeout = 15s

    # If commits take longer than this time a warning is logged
    commit-time-warning = 1s

    # Not used anymore (since 1.0-RC1)
    # wakeup-timeout = 3s

    # Not used anymore (since 1.0-RC1)
    # max-wakeups = 10

    # If set to a finite duration, the consumer will re-send the last committed offsets periodically
    # for all assigned partitions. See https://issues.apache.org/jira/browse/KAFKA-4682.
    commit-refresh-interval = infinite

    # Not used anymore (since 1.0-RC1)
    # wakeup-debug = true

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the KafkaConsumerActor. Some blocking may occur.
    use-dispatcher = "akka.kafka.default-dispatcher"

    # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
    # can be defined in this configuration section.
    kafka-clients {
      # Disable auto-commit by default
      enable.auto.commit = false
    }

    # Time to wait for pending requests when a partition is closed
    wait-close-partition = 500ms

    # Limits the query to Kafka for a topic's position
    position-timeout = 5s

    # When using `AssignmentOffsetsForTimes` subscriptions: timeout for the
    # call to Kafka's API
    offset-for-times-timeout = 5s

    # Timeout for akka.kafka.Metadata requests
    # This value is used instead of Kafka's default from `default.api.timeout.ms`
    # which is 1 minute.
    metadata-request-timeout = 5s

    # Interval for checking that transaction was completed before closing the consumer.
    # Used in the transactional flow for exactly-once-semantics processing.
    eos-draining-check-interval = 30ms

    # Issue warnings when a call to a partition assignment handler method takes
    # longer than this.
    partition-handler-warning = 5s

    # Settings for checking the connection to the Kafka broker. Connection checking uses `listTopics` requests with the timeout
    # configured by `consumer.metadata-request-timeout`
    connection-checker {
      #Flag to turn on connection checker
      enable = true

      # Amount of attempts to be performed after a first connection failure occurs
      # Required, non-negative integer
      max-retries = 3

      # Interval for the connection check. Used as the base for exponential retry.
      check-interval = 15s

      # Check interval multiplier for backoff interval
      # Required, positive number
      backoff-factor = 2.0
    }
  }
  avro = ${radix.avro}
}
